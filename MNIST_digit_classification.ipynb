{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify digits of MNIST database\n",
    "These experiments have been inspired by Hands-on machine learning by Aurlie Geron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using the Estimator API (formerly tf.contrib.learn)\n",
    "Here, I try to use tensorflow's Estimator API, to do a quick check of how well the data and classifier are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivanand\\AppData\\Local\\conda\\conda\\envs\\tensorflowproject\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data between [0, 1] and convert it into appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I use tf.estimator.DNNClassifier Class of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[28 * 28])]\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                     feature_columns=feature_cols)\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train}, y=y_train, num_epochs=40, batch_size=50, shuffle=True)\n",
    "dnn_clf.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and Predict on the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_fun_test = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_valid}, y=y_valid, shuffle=False)\n",
    "evalu = dnn_clf.evaluate(input_fn = input_fun_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_iter  = dnn_clf.predict(input_fn=input_fun_test)\n",
    "y_pred = list(y_pred_iter)\n",
    "y_pred[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using plain TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "with tf.name_scope('my_variables'):\n",
    "    w1 = tf.Variable(initial_value=tf.random_normal([n_inputs, n_hidden1], stddev=2/np.sqrt(n_inputs + n_hidden1)), dtype=tf.float32)\n",
    "    w2 = tf.Variable(initial_value=tf.random_normal([n_hidden1, n_hidden2], stddev=2/np.sqrt(n_hidden1 + n_hidden2)), dtype=tf.float32)\n",
    "    w3 = tf.Variable(initial_value=tf.random_normal([n_hidden2, n_outputs], stddev=2/np.sqrt(n_outputs + n_hidden2)), dtype=tf.float32)\n",
    "    b1= tf.Variable(np.zeros(shape=[n_hidden1]), dtype=tf.float32)\n",
    "    b2 = tf.Variable(np.zeros([n_hidden2]), dtype=tf.float32)\n",
    "    b3 = tf.Variable(np.zeros([n_outputs]), dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"H_values\"):\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    logits = tf.matmul(h2, w3) + b3\n",
    "    \n",
    "with tf.name_scope(\"Loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope('GDescent'):\n",
    "    gD = tf.train.GradientDescentOptimizer(.01)\n",
    "    oper = gD.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "def shuffle_batch(X, y):\n",
    "    rnd_index = np.random.permutation(len(X))\n",
    "    batches = len(X) // batch_size\n",
    "    for index in np.array_split(rnd_index, batches):\n",
    "        X_batch = X[index]\n",
    "        y_batch = y[index]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train ):\n",
    "            sess.run(oper,feed_dict={X:X_batch, y:y_batch})\n",
    "        if i%10 == 0:\n",
    "            print(\"loss \", loss.eval(feed_dict={X:X_train, y:y_train}))\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(i, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "\n",
    "logdir =  log_dir(\"mnist_dnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was interrupted. Continuing at epoch 6\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_deep_mnist_model.ckpt\n",
      "6 Batch accuracy: 0.96 Val accuracy: 0.9534\n",
      "7 Batch accuracy: 0.96 Val accuracy: 0.9588\n",
      "8 Batch accuracy: 0.94 Val accuracy: 0.9592\n",
      "9 Batch accuracy: 0.98 Val accuracy: 0.9622\n",
      "10 Batch accuracy: 0.96 Val accuracy: 0.963\n",
      "loss  0.12588255\n",
      "11 Batch accuracy: 0.96 Val accuracy: 0.9634\n",
      "12 Batch accuracy: 0.98 Val accuracy: 0.9662\n",
      "13 Batch accuracy: 1.0 Val accuracy: 0.9674\n",
      "14 Batch accuracy: 0.94 Val accuracy: 0.9676\n",
      "15 Batch accuracy: 0.94 Val accuracy: 0.97\n",
      "loss  0.0943256\n",
      "16 Batch accuracy: 0.98 Val accuracy: 0.9698\n",
      "17 Batch accuracy: 1.0 Val accuracy: 0.9694\n",
      "18 Batch accuracy: 0.98 Val accuracy: 0.9722\n",
      "19 Batch accuracy: 1.0 Val accuracy: 0.9726\n",
      "20 Batch accuracy: 1.0 Val accuracy: 0.9732\n",
      "loss  0.07208039\n",
      "21 Batch accuracy: 0.98 Val accuracy: 0.9728\n",
      "22 Batch accuracy: 1.0 Val accuracy: 0.9746\n",
      "23 Batch accuracy: 0.98 Val accuracy: 0.9744\n",
      "24 Batch accuracy: 1.0 Val accuracy: 0.9762\n",
      "25 Batch accuracy: 0.98 Val accuracy: 0.9746\n",
      "loss  0.05794836\n",
      "26 Batch accuracy: 0.98 Val accuracy: 0.9748\n",
      "27 Batch accuracy: 0.94 Val accuracy: 0.9764\n",
      "28 Batch accuracy: 1.0 Val accuracy: 0.9762\n",
      "29 Batch accuracy: 1.0 Val accuracy: 0.976\n",
      "30 Batch accuracy: 1.0 Val accuracy: 0.978\n",
      "loss  0.046801288\n",
      "31 Batch accuracy: 1.0 Val accuracy: 0.9772\n",
      "32 Batch accuracy: 1.0 Val accuracy: 0.977\n",
      "33 Batch accuracy: 1.0 Val accuracy: 0.9782\n",
      "34 Batch accuracy: 1.0 Val accuracy: 0.9778\n",
      "35 Batch accuracy: 1.0 Val accuracy: 0.9788\n",
      "loss  0.038007632\n",
      "36 Batch accuracy: 1.0 Val accuracy: 0.9784\n",
      "37 Batch accuracy: 1.0 Val accuracy: 0.9788\n",
      "38 Batch accuracy: 1.0 Val accuracy: 0.9798\n",
      "39 Batch accuracy: 1.0 Val accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    h1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, n_hidden2, name=\"hidden2\",activation=tf.nn.relu )\n",
    "    logits = tf.layers.dense(h2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"Loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "    \n",
    "with tf.name_scope('GDescent'):\n",
    "    gD = tf.train.GradientDescentOptimizer(.01)\n",
    "    oper = gD.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "def shuffle_batch(X, y):\n",
    "    rnd_index = np.random.permutation(len(X))\n",
    "    batches = len(X) // batch_size\n",
    "    for index in np.array_split(rnd_index, batches):\n",
    "        X_batch = X[index]\n",
    "        y_batch = y[index]\n",
    "        yield X_batch, y_batch\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "checkpoint_path = \"/tmp/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_deep_mnist_model\"\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "         \n",
    "    for i in range(start_epoch, n_epochs):\n",
    "        \n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train ):\n",
    "            sess.run(oper, feed_dict={X:X_batch, y:y_batch})\n",
    "        loss_summary_str, accuracy_summary_str = sess.run([loss_summary, accuracy_summary], feed_dict={X:X_train, y:y_train})\n",
    "        file_writer.add_summary(accuracy_summary_str, i)\n",
    "        file_writer.add_summary(loss_summary_str, i)\n",
    "        \n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(i, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n",
    "        \n",
    "        if i%5 == 0:\n",
    "            loss_f = loss.eval(feed_dict={X:X_train, y:y_train})\n",
    "            print(\"loss \", loss_f)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, 'wb') as f:\n",
    "                f.write(b\"%d\"%(i+1))\n",
    "            if loss_f < best_loss: \n",
    "                best_loss = loss_f\n",
    "            else:\n",
    "                epochs_without_progress +=5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    break\n",
    "                \n",
    "    saver.save(sess, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflowproject)",
   "language": "python",
   "name": "tensorflowproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
